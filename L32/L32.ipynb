{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "74407112-816d-404d-8570-d567e5b56319",
   "metadata": {},
   "source": [
    "# 模型验证套路"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fc4022dd-26ca-48cd-8b01-c9f5d901f096",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision\n",
    "from PIL import Image\n",
    "from model import *\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62c947d8-abc7-48b1-95fb-ffc43a5651e3",
   "metadata": {},
   "source": [
    "### torchvision.transforms.Resize()方法"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55f3a0ba-6408-4951-a4eb-2d055ff6a524",
   "metadata": {},
   "source": [
    "```python\n",
    "class torchvision.transforms.Resize(size, interpolation=InterpolationMode.BILINEAR, max_size=None, antialias=True)\n",
    "Resize the input image to the given size. If the image is torch Tensor, it is expected to have […, H, W] shape, where … means a maximum of two leading dimensions\n",
    "\n",
    "Parameters:\n",
    "\n",
    "        size (sequence or int) –\n",
    "\n",
    "        Desired output size. If size is a sequence like (h, w), output size will be matched to this. If size is an int, smaller edge of the image will be matched to this number. i.e, if height > width, then image will be rescaled to (size * height / width, size).\n",
    "\n",
    "        Note\n",
    "\n",
    "        In torchscript mode size as single int is not supported, use a sequence of length 1: [size, ].\n",
    "\n",
    "        interpolation (InterpolationMode) – Desired interpolation enum defined by torchvision.transforms.InterpolationMode. Default is InterpolationMode.BILINEAR. If input is Tensor, only InterpolationMode.NEAREST, InterpolationMode.NEAREST_EXACT, InterpolationMode.BILINEAR and InterpolationMode.BICUBIC are supported. The corresponding Pillow integer constants, e.g. PIL.Image.BILINEAR are accepted as well.\n",
    "\n",
    "        max_size (int, optional) – The maximum allowed for the longer edge of the resized image. If the longer edge of the image is greater than max_size after being resized according to size, size will be overruled so that the longer edge is equal to max_size. As a result, the smaller edge may be shorter than size. This is only supported if size is an int (or a sequence of length 1 in torchscript mode).\n",
    "\n",
    "        antialias (bool, optional) –\n",
    "\n",
    "        Whether to apply antialiasing. It only affects tensors with bilinear or bicubic modes and it is ignored otherwise: on PIL images, antialiasing is always applied on bilinear or bicubic modes; on other modes (for PIL images and tensors), antialiasing makes no sense and this parameter is ignored. Possible values are:\n",
    "\n",
    "            True (default): will apply antialiasing for bilinear or bicubic modes. Other mode aren’t affected. This is probably what you want to use.\n",
    "\n",
    "            False: will not apply antialiasing for tensors on any mode. PIL images are still antialiased on bilinear or bicubic modes, because PIL doesn’t support no antialias.\n",
    "\n",
    "            None: equivalent to False for tensors and True for PIL images. This value exists for legacy reasons and you probably don’t want to use it unless you really know what you are doing.\n",
    "\n",
    "        The default value changed from None to True in v0.17, for the PIL and Tensor backends to be consistent.\n",
    "\n",
    " forward(img)\n",
    "\n",
    "\n",
    "    Parameters:\n",
    "\n",
    "        img (PIL Image or Tensor) – Image to be scaled.\n",
    "    Returns:\n",
    "\n",
    "        Rescaled image.\n",
    "    Return type:\n",
    "\n",
    "        PIL Image or Tensor\n",
    "```\n",
    "\n",
    "**解释如下：**\n",
    "在PyTorch官方文档中，有两个不同的“Parameters”部分，分别用于解释torchvision.transforms.Resize方法的参数和forward方法的参数。这两个方法是在使用图像转换时的不同步骤和目的。\n",
    "+ torchvision.transforms.Resize的Parameters:\n",
    " >size (sequence or int)：指定调整后的目标尺寸。如果size是一个序列（如(h, w)），则输出图像将与此尺寸匹配。如果size是一个整数，则图像的较短边将与此数字匹配，并相应地调整另一边，保持图像的比例不变。   \n",
    "interpolation (InterpolationMode)：指定用于调整大小时的插值方式。默认是双线性插值（BILINEAR）。只有特定的插值模式适用于张量，而Pillow（PIL）支持更多类型的插值。   \n",
    "max_size (int, optional)：设置调整大小后图像较长边的最大允许长度。如果按照size设置后图像的较长边超过max_size，则会重设尺寸以使较长边等于max_size。   \n",
    "antialias (bool, optional)：是否应用抗锯齿。这只影响采用双线性或双三次插值模式的张量，对PIL图像无需设置，因为PIL在这些模式下默认进行抗锯齿处理   \n",
    "\n",
    "+ forward方法的Parameters: img (PIL Image or Tensor)：需要被调整大小的图像。此参数可以是PIL图像或张量格式。   \n",
    ">forward方法是Resize类的实际调用方法，用于执行图像缩放操作。你在使用Resize转换实例对图像进行操作时，通过调用这个方法来将之前设置的参数应用于具体的图像，实现调整大小的功能。\n",
    "\n",
    "简单来说，Resize的参数用于定义如何改变图像的大小（包括尺寸、插值方法等），而forward方法则是用这些定义来具体操作输入的图像。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "34443adb-5532-4078-9150-373ebddad17a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 32, 32])\n",
      "torch.Size([1, 3, 32, 32])\n"
     ]
    }
   ],
   "source": [
    "image_path = \"./dog.jpeg\"                     # 需要先对原先的图片作resize操作，CIFAR10接受的模型输入是32*32\n",
    "image = Image.open(image_path)      \n",
    "\n",
    "image = image.convert('RGB')\n",
    "transfrom = torchvision.transforms.Compose([ torchvision.transforms.Resize((32, 32)),\n",
    "                                                                                                 torchvision.transforms.ToTensor()])\n",
    "\n",
    "image = transfrom(image)\n",
    "print(image.shape)\n",
    "\n",
    "image = torch.reshape(image, (1, 3, 32, 32))\n",
    "print(image.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc491f16-f182-478e-956f-e9b90fd37432",
   "metadata": {},
   "source": [
    "### CPU训练出的模型导入"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "60988bcc-6f9b-44f0-a5d9-4404fcaf379f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tudui(\n",
      "  (model): Sequential(\n",
      "    (0): Conv2d(3, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "    (1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (2): Conv2d(32, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "    (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (4): Conv2d(32, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "    (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (6): Flatten()\n",
      "    (7): Linear(in_features=1024, out_features=64, bias=True)\n",
      "    (8): Linear(in_features=64, out_features=10, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# 这个方法必须是CPU条件下训练出来的模型：\n",
    "model = torch.load(\"../L27_L29/CAFIR10_ckpt_e10.pth\")\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c3d8369a-2be5-48cb-9416-fb2c3a045b42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-2.9093, -2.8773,  1.8954,  2.2320,  0.2233,  3.7644, -0.9736,  1.4850,\n",
      "         -1.8810, -1.9220]])\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    output = model(image)\n",
    "    print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c18b8143-17b5-4540-a399-ac389b4f7de2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([5])\n"
     ]
    }
   ],
   "source": [
    "print(output.argmax(1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "238cac92-7c73-4e3c-8f68-841e5b272ab5",
   "metadata": {},
   "source": [
    "☝ 第五个标签恰好是🐶，说明模型预测正确"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f6f2e0a-e4d4-439e-8500-532c19d57cab",
   "metadata": {},
   "source": [
    "### GPU训练出的模型导入\n",
    "+ **不同pytorch版本下训练出来的模型不能用其他版本的pytorch进行导入推理**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cf5435a0-ebe1-491a-b319-76d42e61f1e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tudui(\n",
      "  (model): Sequential(\n",
      "    (0): Conv2d(3, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "    (1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (2): Conv2d(32, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "    (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (4): Conv2d(32, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "    (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (6): Flatten()\n",
      "    (7): Linear(in_features=1024, out_features=64, bias=True)\n",
      "    (8): Linear(in_features=64, out_features=10, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model = torch.load(\"../L27_L29/CAFIR10_ckpt_e10.pth\", map_location=torch.device('cpu'))              # 只需要在map_location修改映射的device就ok\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1393f385-0d0b-41bc-8535-473f9962fa23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-2.9093, -2.8773,  1.8954,  2.2320,  0.2233,  3.7644, -0.9736,  1.4850,\n",
      "         -1.8810, -1.9220]])\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    output = model(image)\n",
    "    print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ec931236-69de-46d0-990f-4dafd99b1b86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([5])\n"
     ]
    }
   ],
   "source": [
    "print(output.argmax(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e521c18e-c306-4983-b357-2f503a5a44a6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
