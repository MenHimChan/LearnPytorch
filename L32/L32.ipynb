{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "74407112-816d-404d-8570-d567e5b56319",
   "metadata": {},
   "source": [
    "# æ¨¡å‹éªŒè¯å¥—è·¯"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fc4022dd-26ca-48cd-8b01-c9f5d901f096",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision\n",
    "from PIL import Image\n",
    "from model import *\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62c947d8-abc7-48b1-95fb-ffc43a5651e3",
   "metadata": {},
   "source": [
    "### torchvision.transforms.Resize()æ–¹æ³•"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55f3a0ba-6408-4951-a4eb-2d055ff6a524",
   "metadata": {},
   "source": [
    "```python\n",
    "class torchvision.transforms.Resize(size, interpolation=InterpolationMode.BILINEAR, max_size=None, antialias=True)\n",
    "Resize the input image to the given size. If the image is torch Tensor, it is expected to have [â€¦, H, W] shape, where â€¦ means a maximum of two leading dimensions\n",
    "\n",
    "Parameters:\n",
    "\n",
    "        size (sequence or int) â€“\n",
    "\n",
    "        Desired output size. If size is a sequence like (h, w), output size will be matched to this. If size is an int, smaller edge of the image will be matched to this number. i.e, if height > width, then image will be rescaled to (size * height / width, size).\n",
    "\n",
    "        Note\n",
    "\n",
    "        In torchscript mode size as single int is not supported, use a sequence of length 1: [size, ].\n",
    "\n",
    "        interpolation (InterpolationMode) â€“ Desired interpolation enum defined by torchvision.transforms.InterpolationMode. Default is InterpolationMode.BILINEAR. If input is Tensor, only InterpolationMode.NEAREST, InterpolationMode.NEAREST_EXACT, InterpolationMode.BILINEAR and InterpolationMode.BICUBIC are supported. The corresponding Pillow integer constants, e.g. PIL.Image.BILINEAR are accepted as well.\n",
    "\n",
    "        max_size (int, optional) â€“ The maximum allowed for the longer edge of the resized image. If the longer edge of the image is greater than max_size after being resized according to size, size will be overruled so that the longer edge is equal to max_size. As a result, the smaller edge may be shorter than size. This is only supported if size is an int (or a sequence of length 1 in torchscript mode).\n",
    "\n",
    "        antialias (bool, optional) â€“\n",
    "\n",
    "        Whether to apply antialiasing. It only affects tensors with bilinear or bicubic modes and it is ignored otherwise: on PIL images, antialiasing is always applied on bilinear or bicubic modes; on other modes (for PIL images and tensors), antialiasing makes no sense and this parameter is ignored. Possible values are:\n",
    "\n",
    "            True (default): will apply antialiasing for bilinear or bicubic modes. Other mode arenâ€™t affected. This is probably what you want to use.\n",
    "\n",
    "            False: will not apply antialiasing for tensors on any mode. PIL images are still antialiased on bilinear or bicubic modes, because PIL doesnâ€™t support no antialias.\n",
    "\n",
    "            None: equivalent to False for tensors and True for PIL images. This value exists for legacy reasons and you probably donâ€™t want to use it unless you really know what you are doing.\n",
    "\n",
    "        The default value changed from None to True in v0.17, for the PIL and Tensor backends to be consistent.\n",
    "\n",
    " forward(img)\n",
    "\n",
    "\n",
    "    Parameters:\n",
    "\n",
    "        img (PIL Image or Tensor) â€“ Image to be scaled.\n",
    "    Returns:\n",
    "\n",
    "        Rescaled image.\n",
    "    Return type:\n",
    "\n",
    "        PIL Image or Tensor\n",
    "```\n",
    "\n",
    "**è§£é‡Šå¦‚ä¸‹ï¼š**\n",
    "åœ¨PyTorchå®˜æ–¹æ–‡æ¡£ä¸­ï¼Œæœ‰ä¸¤ä¸ªä¸åŒçš„â€œParametersâ€éƒ¨åˆ†ï¼Œåˆ†åˆ«ç”¨äºè§£é‡Štorchvision.transforms.Resizeæ–¹æ³•çš„å‚æ•°å’Œforwardæ–¹æ³•çš„å‚æ•°ã€‚è¿™ä¸¤ä¸ªæ–¹æ³•æ˜¯åœ¨ä½¿ç”¨å›¾åƒè½¬æ¢æ—¶çš„ä¸åŒæ­¥éª¤å’Œç›®çš„ã€‚\n",
    "+ torchvision.transforms.Resizeçš„Parameters:\n",
    " >size (sequence or int)ï¼šæŒ‡å®šè°ƒæ•´åçš„ç›®æ ‡å°ºå¯¸ã€‚å¦‚æœsizeæ˜¯ä¸€ä¸ªåºåˆ—ï¼ˆå¦‚(h, w)ï¼‰ï¼Œåˆ™è¾“å‡ºå›¾åƒå°†ä¸æ­¤å°ºå¯¸åŒ¹é…ã€‚å¦‚æœsizeæ˜¯ä¸€ä¸ªæ•´æ•°ï¼Œåˆ™å›¾åƒçš„è¾ƒçŸ­è¾¹å°†ä¸æ­¤æ•°å­—åŒ¹é…ï¼Œå¹¶ç›¸åº”åœ°è°ƒæ•´å¦ä¸€è¾¹ï¼Œä¿æŒå›¾åƒçš„æ¯”ä¾‹ä¸å˜ã€‚   \n",
    "interpolation (InterpolationMode)ï¼šæŒ‡å®šç”¨äºè°ƒæ•´å¤§å°æ—¶çš„æ’å€¼æ–¹å¼ã€‚é»˜è®¤æ˜¯åŒçº¿æ€§æ’å€¼ï¼ˆBILINEARï¼‰ã€‚åªæœ‰ç‰¹å®šçš„æ’å€¼æ¨¡å¼é€‚ç”¨äºå¼ é‡ï¼Œè€ŒPillowï¼ˆPILï¼‰æ”¯æŒæ›´å¤šç±»å‹çš„æ’å€¼ã€‚   \n",
    "max_size (int, optional)ï¼šè®¾ç½®è°ƒæ•´å¤§å°åå›¾åƒè¾ƒé•¿è¾¹çš„æœ€å¤§å…è®¸é•¿åº¦ã€‚å¦‚æœæŒ‰ç…§sizeè®¾ç½®åå›¾åƒçš„è¾ƒé•¿è¾¹è¶…è¿‡max_sizeï¼Œåˆ™ä¼šé‡è®¾å°ºå¯¸ä»¥ä½¿è¾ƒé•¿è¾¹ç­‰äºmax_sizeã€‚   \n",
    "antialias (bool, optional)ï¼šæ˜¯å¦åº”ç”¨æŠ—é”¯é½¿ã€‚è¿™åªå½±å“é‡‡ç”¨åŒçº¿æ€§æˆ–åŒä¸‰æ¬¡æ’å€¼æ¨¡å¼çš„å¼ é‡ï¼Œå¯¹PILå›¾åƒæ— éœ€è®¾ç½®ï¼Œå› ä¸ºPILåœ¨è¿™äº›æ¨¡å¼ä¸‹é»˜è®¤è¿›è¡ŒæŠ—é”¯é½¿å¤„ç†   \n",
    "\n",
    "+ forwardæ–¹æ³•çš„Parameters: img (PIL Image or Tensor)ï¼šéœ€è¦è¢«è°ƒæ•´å¤§å°çš„å›¾åƒã€‚æ­¤å‚æ•°å¯ä»¥æ˜¯PILå›¾åƒæˆ–å¼ é‡æ ¼å¼ã€‚   \n",
    ">forwardæ–¹æ³•æ˜¯Resizeç±»çš„å®é™…è°ƒç”¨æ–¹æ³•ï¼Œç”¨äºæ‰§è¡Œå›¾åƒç¼©æ”¾æ“ä½œã€‚ä½ åœ¨ä½¿ç”¨Resizeè½¬æ¢å®ä¾‹å¯¹å›¾åƒè¿›è¡Œæ“ä½œæ—¶ï¼Œé€šè¿‡è°ƒç”¨è¿™ä¸ªæ–¹æ³•æ¥å°†ä¹‹å‰è®¾ç½®çš„å‚æ•°åº”ç”¨äºå…·ä½“çš„å›¾åƒï¼Œå®ç°è°ƒæ•´å¤§å°çš„åŠŸèƒ½ã€‚\n",
    "\n",
    "ç®€å•æ¥è¯´ï¼ŒResizeçš„å‚æ•°ç”¨äºå®šä¹‰å¦‚ä½•æ”¹å˜å›¾åƒçš„å¤§å°ï¼ˆåŒ…æ‹¬å°ºå¯¸ã€æ’å€¼æ–¹æ³•ç­‰ï¼‰ï¼Œè€Œforwardæ–¹æ³•åˆ™æ˜¯ç”¨è¿™äº›å®šä¹‰æ¥å…·ä½“æ“ä½œè¾“å…¥çš„å›¾åƒã€‚\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "34443adb-5532-4078-9150-373ebddad17a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 32, 32])\n",
      "torch.Size([1, 3, 32, 32])\n"
     ]
    }
   ],
   "source": [
    "image_path = \"./dog.jpeg\"                     # éœ€è¦å…ˆå¯¹åŸå…ˆçš„å›¾ç‰‡ä½œresizeæ“ä½œï¼ŒCIFAR10æ¥å—çš„æ¨¡å‹è¾“å…¥æ˜¯32*32\n",
    "image = Image.open(image_path)      \n",
    "\n",
    "image = image.convert('RGB')\n",
    "transfrom = torchvision.transforms.Compose([ torchvision.transforms.Resize((32, 32)),\n",
    "                                                                                                 torchvision.transforms.ToTensor()])\n",
    "\n",
    "image = transfrom(image)\n",
    "print(image.shape)\n",
    "\n",
    "image = torch.reshape(image, (1, 3, 32, 32))\n",
    "print(image.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc491f16-f182-478e-956f-e9b90fd37432",
   "metadata": {},
   "source": [
    "### CPUè®­ç»ƒå‡ºçš„æ¨¡å‹å¯¼å…¥"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "60988bcc-6f9b-44f0-a5d9-4404fcaf379f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tudui(\n",
      "  (model): Sequential(\n",
      "    (0): Conv2d(3, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "    (1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (2): Conv2d(32, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "    (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (4): Conv2d(32, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "    (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (6): Flatten()\n",
      "    (7): Linear(in_features=1024, out_features=64, bias=True)\n",
      "    (8): Linear(in_features=64, out_features=10, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# è¿™ä¸ªæ–¹æ³•å¿…é¡»æ˜¯CPUæ¡ä»¶ä¸‹è®­ç»ƒå‡ºæ¥çš„æ¨¡å‹ï¼š\n",
    "model = torch.load(\"../L27_L29/CAFIR10_ckpt_e10.pth\")\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c3d8369a-2be5-48cb-9416-fb2c3a045b42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-2.9093, -2.8773,  1.8954,  2.2320,  0.2233,  3.7644, -0.9736,  1.4850,\n",
      "         -1.8810, -1.9220]])\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    output = model(image)\n",
    "    print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c18b8143-17b5-4540-a399-ac389b4f7de2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([5])\n"
     ]
    }
   ],
   "source": [
    "print(output.argmax(1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "238cac92-7c73-4e3c-8f68-841e5b272ab5",
   "metadata": {},
   "source": [
    "â˜ ç¬¬äº”ä¸ªæ ‡ç­¾æ°å¥½æ˜¯ğŸ¶ï¼Œè¯´æ˜æ¨¡å‹é¢„æµ‹æ­£ç¡®"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f6f2e0a-e4d4-439e-8500-532c19d57cab",
   "metadata": {},
   "source": [
    "### GPUè®­ç»ƒå‡ºçš„æ¨¡å‹å¯¼å…¥\n",
    "+ **ä¸åŒpytorchç‰ˆæœ¬ä¸‹è®­ç»ƒå‡ºæ¥çš„æ¨¡å‹ä¸èƒ½ç”¨å…¶ä»–ç‰ˆæœ¬çš„pytorchè¿›è¡Œå¯¼å…¥æ¨ç†**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cf5435a0-ebe1-491a-b319-76d42e61f1e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tudui(\n",
      "  (model): Sequential(\n",
      "    (0): Conv2d(3, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "    (1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (2): Conv2d(32, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "    (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (4): Conv2d(32, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "    (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (6): Flatten()\n",
      "    (7): Linear(in_features=1024, out_features=64, bias=True)\n",
      "    (8): Linear(in_features=64, out_features=10, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model = torch.load(\"../L27_L29/CAFIR10_ckpt_e10.pth\", map_location=torch.device('cpu'))              # åªéœ€è¦åœ¨map_locationä¿®æ”¹æ˜ å°„çš„deviceå°±ok\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1393f385-0d0b-41bc-8535-473f9962fa23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-2.9093, -2.8773,  1.8954,  2.2320,  0.2233,  3.7644, -0.9736,  1.4850,\n",
      "         -1.8810, -1.9220]])\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    output = model(image)\n",
    "    print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ec931236-69de-46d0-990f-4dafd99b1b86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([5])\n"
     ]
    }
   ],
   "source": [
    "print(output.argmax(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e521c18e-c306-4983-b357-2f503a5a44a6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
