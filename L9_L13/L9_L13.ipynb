{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ed0f05c7-41e8-47ea-98b0-e39164759d19",
   "metadata": {},
   "source": [
    "# Transforms的使用"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc652504-5ac3-4cf5-b3f9-56a2f5431a0c",
   "metadata": {},
   "source": [
    " `transforms.py` 工具箱内的函数主要对图像进行变换，包括数据结构转换、尺度变换、高斯模糊、归一化、流水线、裁剪等。\n",
    "+ .Compose：流水线操作，将多个图像操作步骤整合到一起。\n",
    "+ .ToTensor：将PIL或ndarray类型的图像转换成张量（tensor）类型。\n",
    "+ .Resize：将输入的图像转换成不同的大小。\n",
    "+ .CenterCrop：对输入的图像进行中心裁剪。\n",
    "+ .ToPILImage：将tensor或ndarray类型的图像转换成PIL类型的图像。\n",
    "+ .GaussianBlur：对输入的图像进行高斯模糊处理。   \n",
    "---\n",
    "### 1. 导入方法："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f1165768-8135-4fd1-ab83-086493c627e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import transforms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db71f4ed-0447-4bea-9745-af0ccddd1771",
   "metadata": {},
   "source": [
    "目标： python的用法  ==>  tensor数据类型   \n",
    "通过  transforms.ToTensor  解决：   \n",
    "1. transformsd的使用\n",
    "2. 为什么需要使用Tensor数据类型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c9311fb3-b0a1-48f4-a24f-d30fc37069d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7b85aa3f-8cc0-4541-8a48-c6313c1693d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=768x512 at 0x7FEF65E759B0>\n"
     ]
    }
   ],
   "source": [
    "img_path = \"../dataset/train/ants/0013035.jpg\"\n",
    "img = Image.open(img_path)\n",
    "print(img)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca39c273-b14e-4b74-893d-018c1908fe1d",
   "metadata": {},
   "source": [
    "### 2. transforms.ToTensor方法基本介绍\n",
    "\n",
    "---\n",
    "类注释：\n",
    "```c\n",
    "class ToTensor:\n",
    "    \"\"\"Convert a ``PIL Image`` or ``numpy.ndarray`` to tensor. This transform does not support torchscript.\n",
    "\n",
    "    Converts a PIL Image or numpy.ndarray (H x W x C) in the range\n",
    "    [0, 255] to a torch.FloatTensor of shape (C x H x W) in the range [0.0, 1.0]\n",
    "    if the PIL Image belongs to one of the modes (L, LA, P, I, F, RGB, YCbCr, RGBA, CMYK, 1)\n",
    "    or if the numpy.ndarray has dtype = np.uint8\n",
    "\n",
    "    In the other cases, tensors are returned without scaling.\n",
    "\n",
    "    .. note::\n",
    "        Because the input image is scaled to [0.0, 1.0], this transformation should not be used when\n",
    "        transforming target image masks. See the `references`_ for implementing the transforms for image masks.\n",
    "\n",
    "    .. _references: https://github.com/pytorch/vision/tree/master/references/segmentation\n",
    "    \"\"\"\n",
    "\n",
    "    def __call__(self, pic):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            pic (PIL Image or numpy.ndarray): Image to be converted to tensor.\n",
    "\n",
    "        Returns:\n",
    "            Tensor: Converted image.\n",
    "        \"\"\"\n",
    "        return F.to_tensor(pic)\n",
    "\n",
    "    def __repr__(self):\n",
    "        return self.__class__.__name__ + '()'\n",
    "```   \n",
    "\n",
    "（1)   将 PIL Image 或 numpy.ndarray 转为 tensor    \n",
    "（2）如果 PIL Image 属于 (L, LA, P, I, F, RGB, YCbCr, RGBA, CMYK, 1) 中的一种图像类型，或者 numpy.ndarray 格式数据类型是 np.uint8 ，则将 [0, 255] 的数据转为 [0.0, 1.0] ，也就是说将所有数据除以 255 进行归一化。    \n",
    "（3）将 HWC 的图像格式转为 CHW 的 tensor 格式。CNN训练时需要的数据格式是[N,C,N,W]，也就是说经过 ToTensor() 处理的图像可以直接输入到CNN网络中，不需要再进行reshape。    \n",
    "   (4)   类实现的内部调用__call__方法，which is magic method, 可以像普通函数那样调用这个class下的任何实例，方法如下：   \n",
    "           tensor_trans = transforms.ToTensor()   \n",
    "           tensor_img = tensor_trans(img)\n",
    " \n",
    " #### 2.1 传入PILImage参数\n",
    " ---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "70009567-7530-4de9-b0f7-baebad2683a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[0.3137, 0.3137, 0.3137,  ..., 0.3176, 0.3098, 0.2980],\n",
      "         [0.3176, 0.3176, 0.3176,  ..., 0.3176, 0.3098, 0.2980],\n",
      "         [0.3216, 0.3216, 0.3216,  ..., 0.3137, 0.3098, 0.3020],\n",
      "         ...,\n",
      "         [0.3412, 0.3412, 0.3373,  ..., 0.1725, 0.3725, 0.3529],\n",
      "         [0.3412, 0.3412, 0.3373,  ..., 0.3294, 0.3529, 0.3294],\n",
      "         [0.3412, 0.3412, 0.3373,  ..., 0.3098, 0.3059, 0.3294]],\n",
      "\n",
      "        [[0.5922, 0.5922, 0.5922,  ..., 0.5961, 0.5882, 0.5765],\n",
      "         [0.5961, 0.5961, 0.5961,  ..., 0.5961, 0.5882, 0.5765],\n",
      "         [0.6000, 0.6000, 0.6000,  ..., 0.5922, 0.5882, 0.5804],\n",
      "         ...,\n",
      "         [0.6275, 0.6275, 0.6235,  ..., 0.3608, 0.6196, 0.6157],\n",
      "         [0.6275, 0.6275, 0.6235,  ..., 0.5765, 0.6275, 0.5961],\n",
      "         [0.6275, 0.6275, 0.6235,  ..., 0.6275, 0.6235, 0.6314]],\n",
      "\n",
      "        [[0.9137, 0.9137, 0.9137,  ..., 0.9176, 0.9098, 0.8980],\n",
      "         [0.9176, 0.9176, 0.9176,  ..., 0.9176, 0.9098, 0.8980],\n",
      "         [0.9216, 0.9216, 0.9216,  ..., 0.9137, 0.9098, 0.9020],\n",
      "         ...,\n",
      "         [0.9294, 0.9294, 0.9255,  ..., 0.5529, 0.9216, 0.8941],\n",
      "         [0.9294, 0.9294, 0.9255,  ..., 0.8863, 1.0000, 0.9137],\n",
      "         [0.9294, 0.9294, 0.9255,  ..., 0.9490, 0.9804, 0.9137]]])\n"
     ]
    }
   ],
   "source": [
    "tensor_trans = transforms.ToTensor()\n",
    "tensor_img = tensor_trans(img)\n",
    "print(tensor_img)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f401b5ea-8af6-421c-8a69-979cb5ee62e3",
   "metadata": {},
   "source": [
    "#### 2.2 传入numpy数据格式\n",
    "---\n",
    "需要先安装好opencv-python\n",
    "```bash\n",
    "pip install opencv-python==4.1.2.30\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a946a4b9-4d05-41f2-ab64-86b25cfed077",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "883b8eb1-47c6-4e52-9c5c-94c65e09e53e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "cv_img = cv2.imread(img_path)\n",
    "print(type(cv_img))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6ffce834-6c70-42d0-87da-caaa8bea2235",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.tensorboard import SummaryWriter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "74b1849c-1920-4d09-93dc-a4d349454443",
   "metadata": {},
   "outputs": [],
   "source": [
    "writer = SummaryWriter(\"./logs\")\n",
    "writer.add_image(\"Tensor Image\", tensor_img)\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0a21e07-6b3c-4faf-854b-37ef5ddff856",
   "metadata": {},
   "source": [
    "#### 2.3 为什么要用ToTensor,为什么要用Tensor作为数据类型\n",
    "**Tensor这个类别下内置了很多神经网络训练时所需要用到的方法如：`_backward_hooks`, `device`, `_grad` 等等。**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9705ba1b-53b0-4b63-86d5-d3960688e30c",
   "metadata": {},
   "source": [
    "### 3. .ToTensor方法的使用"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43f04afa-db92-4498-bfca-3ff78a7cfb88",
   "metadata": {},
   "source": [
    "#### 3.1 内置__call__方法测试：\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9cf704ea-1f56-452e-96e5-4b0402779b28",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Person:\n",
    "    \n",
    "    def __call__(self, name):\n",
    "        print(\"__call__\" + \"Hello\" + name)\n",
    "        \n",
    "    def hello(self, name):\n",
    "        print(\"hello\" + name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "35d79962-5634-4633-98b3-4657fbb7a226",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__call__Hellozhangsan\n",
      "hellolisi\n"
     ]
    }
   ],
   "source": [
    "person = Person()          # ==============>  初始化这个类的实例\n",
    "person(\"zhangsan\")     # ==============>  直接用调用函数的方法就能直接调用内置的__call__方法 \n",
    "person.hello(\"lisi\")       # ==============>   一般的方法只能通过.调用"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "099d1b3f-53ef-4dd5-8f1d-cbecddef7eaa",
   "metadata": {},
   "source": [
    "#### 3.2 Full Code\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6bb40f9d-1815-440e-bfab-11e5159e6048",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torchvision import transforms\n",
    "\n",
    "\n",
    "writer = SummaryWriter(\"./logs\")                    # 打开TensorBoard\n",
    "img = Image.open(img_path)                            # convert common image to PILImage\n",
    "\n",
    "trans_totensor = transforms.ToTensor()       #  创建ToTensor实例\n",
    "img_tensor = trans_totensor(img)                 #   convert PILImage to Tensor Image\n",
    "\n",
    "writer.add_image(\"Full code for ToTensor Test\", img_tensor, 5)    # 写入TensorBoard\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19f9d6e2-7bcd-453a-b649-bedf22be409d",
   "metadata": {},
   "source": [
    "### 4. .Normalize方法"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc4b3ac3-51ea-4e98-a300-0980bcb426f5",
   "metadata": {},
   "source": [
    "类注释：   \n",
    "```python\n",
    "class Normalize(torch.nn.Module):\n",
    "    \"\"\"Normalize a tensor image with mean and standard deviation.\n",
    "    This transform does not support PIL Image.\n",
    "    Given mean: ``(mean[1],...,mean[n])`` and std: ``(std[1],..,std[n])`` for ``n``\n",
    "    channels, this transform will normalize each channel of the input\n",
    "    ``torch.*Tensor`` i.e.,\n",
    "    ``output[channel] = (input[channel] - mean[channel]) / std[channel]``\n",
    "\n",
    "    .. note::\n",
    "        This transform acts out of place, i.e., it does not mutate the input tensor.\n",
    "\n",
    "    Args:\n",
    "        mean (sequence): Sequence of means for each channel.\n",
    "        std (sequence): Sequence of standard deviations for each channel.\n",
    "        inplace(bool,optional): Bool to make this operation in-place.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, mean, std, inplace=False):\n",
    "        super().__init__()\n",
    "        self.mean = mean\n",
    "        self.std = std\n",
    "        self.inplace = inplace\n",
    "\n",
    "    def forward(self, tensor: Tensor) -> Tensor:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            tensor (Tensor): Tensor image to be normalized.\n",
    "\n",
    "        Returns:\n",
    "            Tensor: Normalized Tensor image.\n",
    "        \"\"\"\n",
    "        return F.normalize(tensor, self.mean, self.std, self.inplace)\n",
    "\n",
    "    def __repr__(self):\n",
    "        return self.__class__.__name__ + '(mean={0}, std={1})'.format(self.mean, self.std)\n",
    "```   \n",
    "非标准正态分布函数的标准化： $z = \\frac{(x - \\mu)}{\\sigma}$, 对比代码注释给出的公式：  $\\text{output[channel]} = \\frac{(\\text{input[channel]} - \\text{mean[channel]})}{\\text{std[channel]}}$，实际上：\n",
    "**简单来说就是将数据按通道进行计算，将每一个通道的数据先计算出其方差与均值，然后再将其每一个通道内的每一个数据减去均值，再除以方差，得到归一化后的结果。\n",
    "在深度学习图像处理中，标准化处理之后，可以使数据更好的响应激活函数，提高数据的表现力，减少梯度爆炸和梯度消失的出现。**\n",
    "\n",
    "Pytorch图像预处理时，通常使用transforms.Normalize(mean, std)对图像按通道进行标准化，即减去均值，再除以方差。这样做可以加快模型的收敛速度。**其中参数mean和std分别表示图像每个通道的均值和方差序列。**\n",
    "Imagenet数据集的均值和方差为：mean=(0.485, 0.456, 0.406)，std=(0.229, 0.224, 0.225)，因为这是在百万张图像上计算而得的，所以我们通常见到在训练过程中使用它们做标准化。而对于特定的数据集，选择这个值的结果可能并不理想。接下来给出计算特定数据集的均值和方差的方法。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "649443dd-6d1e-4d86-81c0-3bb5c3aa91a8",
   "metadata": {},
   "source": [
    "#### 测试代码："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "383799ac-bd0d-4ed8-9fc8-31ba1983e530",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.3137)\n",
      "tensor(-0.3725)\n"
     ]
    }
   ],
   "source": [
    "print(img_tensor[0][0][0])\n",
    "trans_norm = transforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5])      # 第一个数组是各个通道的均值， 第二个数组是各个通道的标准差\n",
    "img_norm = trans_norm(img_tensor)\n",
    "print(img_norm[0][0][0])\n",
    "writer.add_image(\"Normalization Test\", img_norm)\n",
    "\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78dd2717-d071-4a98-b51c-eaf45a35d16f",
   "metadata": {},
   "source": [
    "### 5. .Resize方法\n",
    "---\n",
    "```python\n",
    "class Resize(torch.nn.Module):\n",
    "\"\"\"Resize the input image to the given size.\n",
    "    If the image is torch Tensor, it is expected\n",
    "    to have [..., H, W] shape, where ... means an arbitrary number of leading dimensions\n",
    "\n",
    "    Args:\n",
    "        size (sequence or int): Desired output size. If size is a sequence like\n",
    "            (h, w), output size will be matched to this. If size is an int,\n",
    "            smaller edge of the image will be matched to this number.\n",
    "            i.e, if height > width, then image will be rescaled to\n",
    "            (size * height / width, size).\n",
    "            In torchscript mode size as single int is not supported, use a sequence of length 1: ``[size, ]``.\n",
    "        interpolation (InterpolationMode): Desired interpolation enum defined by\n",
    "            :class:`torchvision.transforms.InterpolationMode`. Default is ``InterpolationMode.BILINEAR``.\n",
    "            If input is Tensor, only ``InterpolationMode.NEAREST``, ``InterpolationMode.BILINEAR`` and\n",
    "            ``InterpolationMode.BICUBIC`` are supported.\n",
    "            For backward compatibility integer values (e.g. ``PIL.Image.NEAREST``) are still acceptable.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, size, interpolation=InterpolationMode.BILINEAR):\n",
    "        super().__init__()\n",
    "        if not isinstance(size, (int, Sequence)):\n",
    "            raise TypeError(\"Size should be int or sequence. Got {}\".format(type(size)))\n",
    "        if isinstance(size, Sequence) and len(size) not in (1, 2):\n",
    "            raise ValueError(\"If size is a sequence, it should have 1 or 2 values\")\n",
    "        self.size = size\n",
    "\n",
    "        # Backward compatibility with integer value\n",
    "        if isinstance(interpolation, int):\n",
    "            warnings.warn(\n",
    "                \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
    "                \"Please, use InterpolationMode enum.\"\n",
    "            )\n",
    "            interpolation = _interpolation_modes_from_int(interpolation)\n",
    "\n",
    "        self.interpolation = interpolation\n",
    "\n",
    "    def forward(self, img):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            img (PIL Image or Tensor): Image to be scaled.\n",
    "\n",
    "        Returns:\n",
    "            PIL Image or Tensor: Rescaled image.\n",
    "        \"\"\"\n",
    "        return F.resize(img, self.size, self.interpolation)\n",
    "\n",
    "    def __repr__(self):\n",
    "        interpolate_str = self.interpolation.value\n",
    "        return self.__class__.__name__ + '(size={0}, interpolation={1})'.format(self.size, interpolate_str)\n",
    " ```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a1b56454-8e43-4aed-9eca-0dbf4d0cd0dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(768, 512)\n",
      "tensor([[[0.3137, 0.3137, 0.3176,  ..., 0.3137, 0.3137, 0.3020],\n",
      "         [0.3176, 0.3176, 0.3176,  ..., 0.3098, 0.3137, 0.3020],\n",
      "         [0.3216, 0.3216, 0.3176,  ..., 0.3059, 0.3137, 0.3059],\n",
      "         ...,\n",
      "         [0.3412, 0.3373, 0.3373,  ..., 0.0196, 0.2196, 0.3608],\n",
      "         [0.3412, 0.3373, 0.3373,  ..., 0.3490, 0.3373, 0.3373],\n",
      "         [0.3412, 0.3373, 0.3373,  ..., 0.3529, 0.3137, 0.3216]],\n",
      "\n",
      "        [[0.5922, 0.5922, 0.5961,  ..., 0.5922, 0.5922, 0.5804],\n",
      "         [0.5961, 0.5961, 0.5961,  ..., 0.5882, 0.5922, 0.5804],\n",
      "         [0.6000, 0.6000, 0.5961,  ..., 0.5843, 0.5922, 0.5843],\n",
      "         ...,\n",
      "         [0.6275, 0.6235, 0.6235,  ..., 0.1020, 0.4157, 0.6157],\n",
      "         [0.6275, 0.6235, 0.6235,  ..., 0.5373, 0.5882, 0.6078],\n",
      "         [0.6275, 0.6235, 0.6235,  ..., 0.6392, 0.6275, 0.6275]],\n",
      "\n",
      "        [[0.9137, 0.9137, 0.9176,  ..., 0.9137, 0.9137, 0.9020],\n",
      "         [0.9176, 0.9176, 0.9176,  ..., 0.9098, 0.9137, 0.9020],\n",
      "         [0.9216, 0.9216, 0.9176,  ..., 0.9059, 0.9137, 0.9059],\n",
      "         ...,\n",
      "         [0.9294, 0.9255, 0.9255,  ..., 0.1961, 0.6353, 0.9059],\n",
      "         [0.9294, 0.9255, 0.9255,  ..., 0.7922, 0.9098, 0.9451],\n",
      "         [0.9294, 0.9255, 0.9255,  ..., 0.9412, 0.9569, 0.9373]]])\n"
     ]
    }
   ],
   "source": [
    "# Resize - 1 - 双参数 ===================================>   直接将传入的图片缩放成填入的数组的格式 \n",
    "print(img.size) \n",
    "trans_resize = transforms.Resize((512, 512))\n",
    "# img: PIL  =>  resize =>  img_resize : PIL \n",
    "img_resize = trans_resize(img)\n",
    "# img_resize : PIL  =>  totensor  =>   img_resize  tensor\n",
    "img_resize = trans_totensor(img_resize)\n",
    "print(img_resize)\n",
    "\n",
    "writer.add_image(\"Image Resize Test\", img_resize, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d57d268e-3476-4341-9183-d9dec2f111b0",
   "metadata": {},
   "source": [
    "### 6. .Compose方法\n",
    "---\n",
    "```python\n",
    "class Compose:\n",
    "    \"\"\"Composes several transforms together. This transform does not support torchscript.\n",
    "    Please, see the note below.\n",
    "\n",
    "    Args:\n",
    "        transforms (list of ``Transform`` objects): list of transforms to compose.\n",
    "\n",
    "    Example:\n",
    "        >>> transforms.Compose([\n",
    "        >>>     transforms.CenterCrop(10),\n",
    "        >>>     transforms.ToTensor(),\n",
    "        >>> ])\n",
    "\n",
    "    .. note::\n",
    "        In order to script the transformations, please use ``torch.nn.Sequential`` as below.\n",
    "\n",
    "        >>> transforms = torch.nn.Sequential(\n",
    "        >>>     transforms.CenterCrop(10),\n",
    "        >>>     transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225)),\n",
    "        >>> )\n",
    "        >>> scripted_transforms = torch.jit.script(transforms)\n",
    "\n",
    "        Make sure to use only scriptable transformations, i.e. that work with ``torch.Tensor``, does not require\n",
    "        `lambda` functions or ``PIL.Image``.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, transforms):\n",
    "        self.transforms = transforms\n",
    "\n",
    "    def __call__(self, img):\n",
    "        for t in self.transforms:\n",
    "            img = t(img)\n",
    "        return img\n",
    "\n",
    "    def __repr__(self):\n",
    "        format_string = self.__class__.__name__ + '('\n",
    "        for t in self.transforms:\n",
    "            format_string += '\\n'\n",
    "            format_string += '    {0}'.format(t)\n",
    "        format_string += '\\n)'\n",
    "        return format_string\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "03e84f43-d21c-4002-9670-f494b15e7972",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compose相当于就是把transforms的很多操作打包在一起再对输入img对象进行处理\n",
    "# 后面参数的输入是前一个参数的输出\n",
    "trans_resize_2 = transforms.Resize(512)                                                                          #  Resize只填入一个参数的时候的缩放方式是不改变长宽比的等比缩放，并且将较短的边缩放至512\n",
    "trans_compose = transforms.Compose([trans_resize_2,  trans_totensor])      #  img  ===>  .Resize(512)  ===>  .ToTensor  ===>  img_resize_2\n",
    "img_resize_2 = trans_compose(img)\n",
    "writer.add_image(\"Compose Test\", img_resize_2, 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "008cb196-ecf8-4781-8e50-180fd4ba7b75",
   "metadata": {},
   "source": [
    "### 7. .RandomCrop方法\n",
    "---\n",
    "```python\n",
    "class RandomCrop(torch.nn.Module):\n",
    "    \"\"\"Crop the given image at a random location.\n",
    "    If the image is torch Tensor, it is expected\n",
    "    to have [..., H, W] shape, where ... means an arbitrary number of leading dimensions,\n",
    "    but if non-constant padding is used, the input is expected to have at most 2 leading dimensions\n",
    "\n",
    "    Args:\n",
    "        size (sequence or int): Desired output size of the crop. If size is an\n",
    "            int instead of sequence like (h, w), a square crop (size, size) is\n",
    "            made. If provided a sequence of length 1, it will be interpreted as (size[0], size[0]).\n",
    "        padding (int or sequence, optional): Optional padding on each border\n",
    "            of the image. Default is None. If a single int is provided this\n",
    "            is used to pad all borders. If sequence of length 2 is provided this is the padding\n",
    "            on left/right and top/bottom respectively. If a sequence of length 4 is provided\n",
    "            this is the padding for the left, top, right and bottom borders respectively.\n",
    "            In torchscript mode padding as single int is not supported, use a sequence of length 1: ``[padding, ]``.\n",
    "        pad_if_needed (boolean): It will pad the image if smaller than the\n",
    "            desired size to avoid raising an exception. Since cropping is done\n",
    "            after padding, the padding seems to be done at a random offset.\n",
    "        fill (number or str or tuple): Pixel fill value for constant fill. Default is 0. If a tuple of\n",
    "            length 3, it is used to fill R, G, B channels respectively.\n",
    "            This value is only used when the padding_mode is constant.\n",
    "            Only number is supported for torch Tensor.\n",
    "            Only int or str or tuple value is supported for PIL Image.\n",
    "        padding_mode (str): Type of padding. Should be: constant, edge, reflect or symmetric. Default is constant.\n",
    "\n",
    "             - constant: pads with a constant value, this value is specified with fill\n",
    "\n",
    "             - edge: pads with the last value on the edge of the image\n",
    "\n",
    "             - reflect: pads with reflection of image (without repeating the last value on the edge)\n",
    "\n",
    "                padding [1, 2, 3, 4] with 2 elements on both sides in reflect mode\n",
    "                will result in [3, 2, 1, 2, 3, 4, 3, 2]\n",
    "\n",
    "             - symmetric: pads with reflection of image (repeating the last value on the edge)\n",
    "\n",
    "                padding [1, 2, 3, 4] with 2 elements on both sides in symmetric mode\n",
    "                will result in [2, 1, 1, 2, 3, 4, 4, 3]\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    @staticmethod\n",
    "    def get_params(img: Tensor, output_size: Tuple[int, int]) -> Tuple[int, int, int, int]:\n",
    "        \"\"\"Get parameters for ``crop`` for a random crop.\n",
    "\n",
    "        Args:\n",
    "            img (PIL Image or Tensor): Image to be cropped.\n",
    "            output_size (tuple): Expected output size of the crop.\n",
    "\n",
    "        Returns:\n",
    "            tuple: params (i, j, h, w) to be passed to ``crop`` for random crop.\n",
    "        \"\"\"\n",
    "        w, h = F._get_image_size(img)\n",
    "        th, tw = output_size\n",
    "\n",
    "        if h + 1 < th or w + 1 < tw:\n",
    "            raise ValueError(\n",
    "                \"Required crop size {} is larger then input image size {}\".format((th, tw), (h, w))\n",
    "            )\n",
    "\n",
    "        if w == tw and h == th:\n",
    "            return 0, 0, h, w\n",
    "\n",
    "        i = torch.randint(0, h - th + 1, size=(1, )).item()\n",
    "        j = torch.randint(0, w - tw + 1, size=(1, )).item()\n",
    "        return i, j, th, tw\n",
    "\n",
    "    def __init__(self, size, padding=None, pad_if_needed=False, fill=0, padding_mode=\"constant\"):\n",
    "        super().__init__()\n",
    "\n",
    "        self.size = tuple(_setup_size(\n",
    "            size, error_msg=\"Please provide only two dimensions (h, w) for size.\"\n",
    "        ))\n",
    "\n",
    "        self.padding = padding\n",
    "        self.pad_if_needed = pad_if_needed\n",
    "        self.fill = fill\n",
    "        self.padding_mode = padding_mode\n",
    "\n",
    "    def forward(self, img):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            img (PIL Image or Tensor): Image to be cropped.\n",
    "\n",
    "        Returns:\n",
    "            PIL Image or Tensor: Cropped image.\n",
    "        \"\"\"\n",
    "        if self.padding is not None:\n",
    "            img = F.pad(img, self.padding, self.fill, self.padding_mode)\n",
    "\n",
    "        width, height = F._get_image_size(img)\n",
    "        # pad the width if needed\n",
    "        if self.pad_if_needed and width < self.size[1]:\n",
    "            padding = [self.size[1] - width, 0]\n",
    "            img = F.pad(img, padding, self.fill, self.padding_mode)\n",
    "        # pad the height if needed\n",
    "        if self.pad_if_needed and height < self.size[0]:\n",
    "            padding = [0, self.size[0] - height]\n",
    "            img = F.pad(img, padding, self.fill, self.padding_mode)\n",
    "\n",
    "        i, j, h, w = self.get_params(img, self.size)\n",
    "\n",
    "        return F.crop(img, i, j, h, w)\n",
    "\n",
    "    def __repr__(self):\n",
    "        return self.__class__.__name__ + \"(size={0}, padding={1})\".format(self.size, self.padding)\n",
    "    \n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "b64b5cb0-e078-4b2b-9ae2-5f8ce6080185",
   "metadata": {},
   "outputs": [],
   "source": [
    "# RandomCrop\n",
    "trans_randomcrop = transforms.RandomCrop((200, 400))\n",
    "trans_compose2 = transforms.Compose([trans_randomcrop, trans_totensor])\n",
    "for _ in range(10):\n",
    "    img_crop = trans_compose2(img)\n",
    "    writer.add_image(\"RandomCrop Test\", img_crop, _)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c84c0278-e683-4857-9cdc-436d9cb2de03",
   "metadata": {},
   "source": [
    "*tips: TensorBoard,故名思义, 🉐输入tensor格式的数据才能进行现实*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9269119e-7e9b-4ba1-9805-7f53498cba40",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  },
  "toc-autonumbering": false,
  "toc-showmarkdowntxt": false
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
