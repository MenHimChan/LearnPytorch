{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ed0f05c7-41e8-47ea-98b0-e39164759d19",
   "metadata": {},
   "source": [
    "# Transformsçš„ä½¿ç”¨"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc652504-5ac3-4cf5-b3f9-56a2f5431a0c",
   "metadata": {},
   "source": [
    " `transforms.py` å·¥å…·ç®±å†…çš„å‡½æ•°ä¸»è¦å¯¹å›¾åƒè¿›è¡Œå˜æ¢ï¼ŒåŒ…æ‹¬æ•°æ®ç»“æ„è½¬æ¢ã€å°ºåº¦å˜æ¢ã€é«˜æ–¯æ¨¡ç³Šã€å½’ä¸€åŒ–ã€æµæ°´çº¿ã€è£å‰ªç­‰ã€‚\n",
    "+ .Composeï¼šæµæ°´çº¿æ“ä½œï¼Œå°†å¤šä¸ªå›¾åƒæ“ä½œæ­¥éª¤æ•´åˆåˆ°ä¸€èµ·ã€‚\n",
    "+ .ToTensorï¼šå°†PILæˆ–ndarrayç±»å‹çš„å›¾åƒè½¬æ¢æˆå¼ é‡ï¼ˆtensorï¼‰ç±»å‹ã€‚\n",
    "+ .Resizeï¼šå°†è¾“å…¥çš„å›¾åƒè½¬æ¢æˆä¸åŒçš„å¤§å°ã€‚\n",
    "+ .CenterCropï¼šå¯¹è¾“å…¥çš„å›¾åƒè¿›è¡Œä¸­å¿ƒè£å‰ªã€‚\n",
    "+ .ToPILImageï¼šå°†tensoræˆ–ndarrayç±»å‹çš„å›¾åƒè½¬æ¢æˆPILç±»å‹çš„å›¾åƒã€‚\n",
    "+ .GaussianBlurï¼šå¯¹è¾“å…¥çš„å›¾åƒè¿›è¡Œé«˜æ–¯æ¨¡ç³Šå¤„ç†ã€‚   \n",
    "---\n",
    "### 1. å¯¼å…¥æ–¹æ³•ï¼š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f1165768-8135-4fd1-ab83-086493c627e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import transforms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db71f4ed-0447-4bea-9745-af0ccddd1771",
   "metadata": {},
   "source": [
    "ç›®æ ‡ï¼š pythonçš„ç”¨æ³•  ==>  tensoræ•°æ®ç±»å‹   \n",
    "é€šè¿‡  transforms.ToTensor  è§£å†³ï¼š   \n",
    "1. transformsdçš„ä½¿ç”¨\n",
    "2. ä¸ºä»€ä¹ˆéœ€è¦ä½¿ç”¨Tensoræ•°æ®ç±»å‹"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c9311fb3-b0a1-48f4-a24f-d30fc37069d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7b85aa3f-8cc0-4541-8a48-c6313c1693d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=768x512 at 0x7FEF65E759B0>\n"
     ]
    }
   ],
   "source": [
    "img_path = \"../dataset/train/ants/0013035.jpg\"\n",
    "img = Image.open(img_path)\n",
    "print(img)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca39c273-b14e-4b74-893d-018c1908fe1d",
   "metadata": {},
   "source": [
    "### 2. transforms.ToTensoræ–¹æ³•åŸºæœ¬ä»‹ç»\n",
    "\n",
    "---\n",
    "ç±»æ³¨é‡Šï¼š\n",
    "```c\n",
    "class ToTensor:\n",
    "    \"\"\"Convert a ``PIL Image`` or ``numpy.ndarray`` to tensor. This transform does not support torchscript.\n",
    "\n",
    "    Converts a PIL Image or numpy.ndarray (H x W x C) in the range\n",
    "    [0, 255] to a torch.FloatTensor of shape (C x H x W) in the range [0.0, 1.0]\n",
    "    if the PIL Image belongs to one of the modes (L, LA, P, I, F, RGB, YCbCr, RGBA, CMYK, 1)\n",
    "    or if the numpy.ndarray has dtype = np.uint8\n",
    "\n",
    "    In the other cases, tensors are returned without scaling.\n",
    "\n",
    "    .. note::\n",
    "        Because the input image is scaled to [0.0, 1.0], this transformation should not be used when\n",
    "        transforming target image masks. See the `references`_ for implementing the transforms for image masks.\n",
    "\n",
    "    .. _references: https://github.com/pytorch/vision/tree/master/references/segmentation\n",
    "    \"\"\"\n",
    "\n",
    "    def __call__(self, pic):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            pic (PIL Image or numpy.ndarray): Image to be converted to tensor.\n",
    "\n",
    "        Returns:\n",
    "            Tensor: Converted image.\n",
    "        \"\"\"\n",
    "        return F.to_tensor(pic)\n",
    "\n",
    "    def __repr__(self):\n",
    "        return self.__class__.__name__ + '()'\n",
    "```   \n",
    "\n",
    "ï¼ˆ1)   å°† PIL Image æˆ– numpy.ndarray è½¬ä¸º tensor    \n",
    "ï¼ˆ2ï¼‰å¦‚æœ PIL Image å±äº (L, LA, P, I, F, RGB, YCbCr, RGBA, CMYK, 1) ä¸­çš„ä¸€ç§å›¾åƒç±»å‹ï¼Œæˆ–è€… numpy.ndarray æ ¼å¼æ•°æ®ç±»å‹æ˜¯ np.uint8 ï¼Œåˆ™å°† [0, 255] çš„æ•°æ®è½¬ä¸º [0.0, 1.0] ï¼Œä¹Ÿå°±æ˜¯è¯´å°†æ‰€æœ‰æ•°æ®é™¤ä»¥ 255 è¿›è¡Œå½’ä¸€åŒ–ã€‚    \n",
    "ï¼ˆ3ï¼‰å°† HWC çš„å›¾åƒæ ¼å¼è½¬ä¸º CHW çš„ tensor æ ¼å¼ã€‚CNNè®­ç»ƒæ—¶éœ€è¦çš„æ•°æ®æ ¼å¼æ˜¯[N,C,N,W]ï¼Œä¹Ÿå°±æ˜¯è¯´ç»è¿‡ ToTensor() å¤„ç†çš„å›¾åƒå¯ä»¥ç›´æ¥è¾“å…¥åˆ°CNNç½‘ç»œä¸­ï¼Œä¸éœ€è¦å†è¿›è¡Œreshapeã€‚    \n",
    "   (4)   ç±»å®ç°çš„å†…éƒ¨è°ƒç”¨__call__æ–¹æ³•ï¼Œwhich is magic method, å¯ä»¥åƒæ™®é€šå‡½æ•°é‚£æ ·è°ƒç”¨è¿™ä¸ªclassä¸‹çš„ä»»ä½•å®ä¾‹ï¼Œæ–¹æ³•å¦‚ä¸‹ï¼š   \n",
    "           tensor_trans = transforms.ToTensor()   \n",
    "           tensor_img = tensor_trans(img)\n",
    " \n",
    " #### 2.1 ä¼ å…¥PILImageå‚æ•°\n",
    " ---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "70009567-7530-4de9-b0f7-baebad2683a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[0.3137, 0.3137, 0.3137,  ..., 0.3176, 0.3098, 0.2980],\n",
      "         [0.3176, 0.3176, 0.3176,  ..., 0.3176, 0.3098, 0.2980],\n",
      "         [0.3216, 0.3216, 0.3216,  ..., 0.3137, 0.3098, 0.3020],\n",
      "         ...,\n",
      "         [0.3412, 0.3412, 0.3373,  ..., 0.1725, 0.3725, 0.3529],\n",
      "         [0.3412, 0.3412, 0.3373,  ..., 0.3294, 0.3529, 0.3294],\n",
      "         [0.3412, 0.3412, 0.3373,  ..., 0.3098, 0.3059, 0.3294]],\n",
      "\n",
      "        [[0.5922, 0.5922, 0.5922,  ..., 0.5961, 0.5882, 0.5765],\n",
      "         [0.5961, 0.5961, 0.5961,  ..., 0.5961, 0.5882, 0.5765],\n",
      "         [0.6000, 0.6000, 0.6000,  ..., 0.5922, 0.5882, 0.5804],\n",
      "         ...,\n",
      "         [0.6275, 0.6275, 0.6235,  ..., 0.3608, 0.6196, 0.6157],\n",
      "         [0.6275, 0.6275, 0.6235,  ..., 0.5765, 0.6275, 0.5961],\n",
      "         [0.6275, 0.6275, 0.6235,  ..., 0.6275, 0.6235, 0.6314]],\n",
      "\n",
      "        [[0.9137, 0.9137, 0.9137,  ..., 0.9176, 0.9098, 0.8980],\n",
      "         [0.9176, 0.9176, 0.9176,  ..., 0.9176, 0.9098, 0.8980],\n",
      "         [0.9216, 0.9216, 0.9216,  ..., 0.9137, 0.9098, 0.9020],\n",
      "         ...,\n",
      "         [0.9294, 0.9294, 0.9255,  ..., 0.5529, 0.9216, 0.8941],\n",
      "         [0.9294, 0.9294, 0.9255,  ..., 0.8863, 1.0000, 0.9137],\n",
      "         [0.9294, 0.9294, 0.9255,  ..., 0.9490, 0.9804, 0.9137]]])\n"
     ]
    }
   ],
   "source": [
    "tensor_trans = transforms.ToTensor()\n",
    "tensor_img = tensor_trans(img)\n",
    "print(tensor_img)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f401b5ea-8af6-421c-8a69-979cb5ee62e3",
   "metadata": {},
   "source": [
    "#### 2.2 ä¼ å…¥numpyæ•°æ®æ ¼å¼\n",
    "---\n",
    "éœ€è¦å…ˆå®‰è£…å¥½opencv-python\n",
    "```bash\n",
    "pip install opencv-python==4.1.2.30\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a946a4b9-4d05-41f2-ab64-86b25cfed077",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "883b8eb1-47c6-4e52-9c5c-94c65e09e53e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "cv_img = cv2.imread(img_path)\n",
    "print(type(cv_img))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6ffce834-6c70-42d0-87da-caaa8bea2235",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.tensorboard import SummaryWriter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "74b1849c-1920-4d09-93dc-a4d349454443",
   "metadata": {},
   "outputs": [],
   "source": [
    "writer = SummaryWriter(\"./logs\")\n",
    "writer.add_image(\"Tensor Image\", tensor_img)\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0a21e07-6b3c-4faf-854b-37ef5ddff856",
   "metadata": {},
   "source": [
    "#### 2.3 ä¸ºä»€ä¹ˆè¦ç”¨ToTensor,ä¸ºä»€ä¹ˆè¦ç”¨Tensorä½œä¸ºæ•°æ®ç±»å‹\n",
    "**Tensorè¿™ä¸ªç±»åˆ«ä¸‹å†…ç½®äº†å¾ˆå¤šç¥ç»ç½‘ç»œè®­ç»ƒæ—¶æ‰€éœ€è¦ç”¨åˆ°çš„æ–¹æ³•å¦‚ï¼š`_backward_hooks`, `device`, `_grad` ç­‰ç­‰ã€‚**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9705ba1b-53b0-4b63-86d5-d3960688e30c",
   "metadata": {},
   "source": [
    "### 3. .ToTensoræ–¹æ³•çš„ä½¿ç”¨"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43f04afa-db92-4498-bfca-3ff78a7cfb88",
   "metadata": {},
   "source": [
    "#### 3.1 å†…ç½®__call__æ–¹æ³•æµ‹è¯•ï¼š\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9cf704ea-1f56-452e-96e5-4b0402779b28",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Person:\n",
    "    \n",
    "    def __call__(self, name):\n",
    "        print(\"__call__\" + \"Hello\" + name)\n",
    "        \n",
    "    def hello(self, name):\n",
    "        print(\"hello\" + name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "35d79962-5634-4633-98b3-4657fbb7a226",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__call__Hellozhangsan\n",
      "hellolisi\n"
     ]
    }
   ],
   "source": [
    "person = Person()          # ==============>  åˆå§‹åŒ–è¿™ä¸ªç±»çš„å®ä¾‹\n",
    "person(\"zhangsan\")     # ==============>  ç›´æ¥ç”¨è°ƒç”¨å‡½æ•°çš„æ–¹æ³•å°±èƒ½ç›´æ¥è°ƒç”¨å†…ç½®çš„__call__æ–¹æ³• \n",
    "person.hello(\"lisi\")       # ==============>   ä¸€èˆ¬çš„æ–¹æ³•åªèƒ½é€šè¿‡.è°ƒç”¨"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "099d1b3f-53ef-4dd5-8f1d-cbecddef7eaa",
   "metadata": {},
   "source": [
    "#### 3.2 Full Code\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6bb40f9d-1815-440e-bfab-11e5159e6048",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torchvision import transforms\n",
    "\n",
    "\n",
    "writer = SummaryWriter(\"./logs\")                    # æ‰“å¼€TensorBoard\n",
    "img = Image.open(img_path)                            # convert common image to PILImage\n",
    "\n",
    "trans_totensor = transforms.ToTensor()       #  åˆ›å»ºToTensorå®ä¾‹\n",
    "img_tensor = trans_totensor(img)                 #   convert PILImage to Tensor Image\n",
    "\n",
    "writer.add_image(\"Full code for ToTensor Test\", img_tensor, 5)    # å†™å…¥TensorBoard\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19f9d6e2-7bcd-453a-b649-bedf22be409d",
   "metadata": {},
   "source": [
    "### 4. .Normalizeæ–¹æ³•"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc4b3ac3-51ea-4e98-a300-0980bcb426f5",
   "metadata": {},
   "source": [
    "ç±»æ³¨é‡Šï¼š   \n",
    "```python\n",
    "class Normalize(torch.nn.Module):\n",
    "    \"\"\"Normalize a tensor image with mean and standard deviation.\n",
    "    This transform does not support PIL Image.\n",
    "    Given mean: ``(mean[1],...,mean[n])`` and std: ``(std[1],..,std[n])`` for ``n``\n",
    "    channels, this transform will normalize each channel of the input\n",
    "    ``torch.*Tensor`` i.e.,\n",
    "    ``output[channel] = (input[channel] - mean[channel]) / std[channel]``\n",
    "\n",
    "    .. note::\n",
    "        This transform acts out of place, i.e., it does not mutate the input tensor.\n",
    "\n",
    "    Args:\n",
    "        mean (sequence): Sequence of means for each channel.\n",
    "        std (sequence): Sequence of standard deviations for each channel.\n",
    "        inplace(bool,optional): Bool to make this operation in-place.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, mean, std, inplace=False):\n",
    "        super().__init__()\n",
    "        self.mean = mean\n",
    "        self.std = std\n",
    "        self.inplace = inplace\n",
    "\n",
    "    def forward(self, tensor: Tensor) -> Tensor:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            tensor (Tensor): Tensor image to be normalized.\n",
    "\n",
    "        Returns:\n",
    "            Tensor: Normalized Tensor image.\n",
    "        \"\"\"\n",
    "        return F.normalize(tensor, self.mean, self.std, self.inplace)\n",
    "\n",
    "    def __repr__(self):\n",
    "        return self.__class__.__name__ + '(mean={0}, std={1})'.format(self.mean, self.std)\n",
    "```   \n",
    "éæ ‡å‡†æ­£æ€åˆ†å¸ƒå‡½æ•°çš„æ ‡å‡†åŒ–ï¼š $z = \\frac{(x - \\mu)}{\\sigma}$, å¯¹æ¯”ä»£ç æ³¨é‡Šç»™å‡ºçš„å…¬å¼ï¼š  $\\text{output[channel]} = \\frac{(\\text{input[channel]} - \\text{mean[channel]})}{\\text{std[channel]}}$ï¼Œå®é™…ä¸Šï¼š\n",
    "**ç®€å•æ¥è¯´å°±æ˜¯å°†æ•°æ®æŒ‰é€šé“è¿›è¡Œè®¡ç®—ï¼Œå°†æ¯ä¸€ä¸ªé€šé“çš„æ•°æ®å…ˆè®¡ç®—å‡ºå…¶æ–¹å·®ä¸å‡å€¼ï¼Œç„¶åå†å°†å…¶æ¯ä¸€ä¸ªé€šé“å†…çš„æ¯ä¸€ä¸ªæ•°æ®å‡å»å‡å€¼ï¼Œå†é™¤ä»¥æ–¹å·®ï¼Œå¾—åˆ°å½’ä¸€åŒ–åçš„ç»“æœã€‚\n",
    "åœ¨æ·±åº¦å­¦ä¹ å›¾åƒå¤„ç†ä¸­ï¼Œæ ‡å‡†åŒ–å¤„ç†ä¹‹åï¼Œå¯ä»¥ä½¿æ•°æ®æ›´å¥½çš„å“åº”æ¿€æ´»å‡½æ•°ï¼Œæé«˜æ•°æ®çš„è¡¨ç°åŠ›ï¼Œå‡å°‘æ¢¯åº¦çˆ†ç‚¸å’Œæ¢¯åº¦æ¶ˆå¤±çš„å‡ºç°ã€‚**\n",
    "\n",
    "Pytorchå›¾åƒé¢„å¤„ç†æ—¶ï¼Œé€šå¸¸ä½¿ç”¨transforms.Normalize(mean, std)å¯¹å›¾åƒæŒ‰é€šé“è¿›è¡Œæ ‡å‡†åŒ–ï¼Œå³å‡å»å‡å€¼ï¼Œå†é™¤ä»¥æ–¹å·®ã€‚è¿™æ ·åšå¯ä»¥åŠ å¿«æ¨¡å‹çš„æ”¶æ•›é€Ÿåº¦ã€‚**å…¶ä¸­å‚æ•°meanå’Œstdåˆ†åˆ«è¡¨ç¤ºå›¾åƒæ¯ä¸ªé€šé“çš„å‡å€¼å’Œæ–¹å·®åºåˆ—ã€‚**\n",
    "Imagenetæ•°æ®é›†çš„å‡å€¼å’Œæ–¹å·®ä¸ºï¼šmean=(0.485, 0.456, 0.406)ï¼Œstd=(0.229, 0.224, 0.225)ï¼Œå› ä¸ºè¿™æ˜¯åœ¨ç™¾ä¸‡å¼ å›¾åƒä¸Šè®¡ç®—è€Œå¾—çš„ï¼Œæ‰€ä»¥æˆ‘ä»¬é€šå¸¸è§åˆ°åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ä½¿ç”¨å®ƒä»¬åšæ ‡å‡†åŒ–ã€‚è€Œå¯¹äºç‰¹å®šçš„æ•°æ®é›†ï¼Œé€‰æ‹©è¿™ä¸ªå€¼çš„ç»“æœå¯èƒ½å¹¶ä¸ç†æƒ³ã€‚æ¥ä¸‹æ¥ç»™å‡ºè®¡ç®—ç‰¹å®šæ•°æ®é›†çš„å‡å€¼å’Œæ–¹å·®çš„æ–¹æ³•ã€‚"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "649443dd-6d1e-4d86-81c0-3bb5c3aa91a8",
   "metadata": {},
   "source": [
    "#### æµ‹è¯•ä»£ç ï¼š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "383799ac-bd0d-4ed8-9fc8-31ba1983e530",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.3137)\n",
      "tensor(-0.3725)\n"
     ]
    }
   ],
   "source": [
    "print(img_tensor[0][0][0])\n",
    "trans_norm = transforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5])      # ç¬¬ä¸€ä¸ªæ•°ç»„æ˜¯å„ä¸ªé€šé“çš„å‡å€¼ï¼Œ ç¬¬äºŒä¸ªæ•°ç»„æ˜¯å„ä¸ªé€šé“çš„æ ‡å‡†å·®\n",
    "img_norm = trans_norm(img_tensor)\n",
    "print(img_norm[0][0][0])\n",
    "writer.add_image(\"Normalization Test\", img_norm)\n",
    "\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78dd2717-d071-4a98-b51c-eaf45a35d16f",
   "metadata": {},
   "source": [
    "### 5. .Resizeæ–¹æ³•\n",
    "---\n",
    "```python\n",
    "class Resize(torch.nn.Module):\n",
    "\"\"\"Resize the input image to the given size.\n",
    "    If the image is torch Tensor, it is expected\n",
    "    to have [..., H, W] shape, where ... means an arbitrary number of leading dimensions\n",
    "\n",
    "    Args:\n",
    "        size (sequence or int): Desired output size. If size is a sequence like\n",
    "            (h, w), output size will be matched to this. If size is an int,\n",
    "            smaller edge of the image will be matched to this number.\n",
    "            i.e, if height > width, then image will be rescaled to\n",
    "            (size * height / width, size).\n",
    "            In torchscript mode size as single int is not supported, use a sequence of length 1: ``[size, ]``.\n",
    "        interpolation (InterpolationMode): Desired interpolation enum defined by\n",
    "            :class:`torchvision.transforms.InterpolationMode`. Default is ``InterpolationMode.BILINEAR``.\n",
    "            If input is Tensor, only ``InterpolationMode.NEAREST``, ``InterpolationMode.BILINEAR`` and\n",
    "            ``InterpolationMode.BICUBIC`` are supported.\n",
    "            For backward compatibility integer values (e.g. ``PIL.Image.NEAREST``) are still acceptable.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, size, interpolation=InterpolationMode.BILINEAR):\n",
    "        super().__init__()\n",
    "        if not isinstance(size, (int, Sequence)):\n",
    "            raise TypeError(\"Size should be int or sequence. Got {}\".format(type(size)))\n",
    "        if isinstance(size, Sequence) and len(size) not in (1, 2):\n",
    "            raise ValueError(\"If size is a sequence, it should have 1 or 2 values\")\n",
    "        self.size = size\n",
    "\n",
    "        # Backward compatibility with integer value\n",
    "        if isinstance(interpolation, int):\n",
    "            warnings.warn(\n",
    "                \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
    "                \"Please, use InterpolationMode enum.\"\n",
    "            )\n",
    "            interpolation = _interpolation_modes_from_int(interpolation)\n",
    "\n",
    "        self.interpolation = interpolation\n",
    "\n",
    "    def forward(self, img):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            img (PIL Image or Tensor): Image to be scaled.\n",
    "\n",
    "        Returns:\n",
    "            PIL Image or Tensor: Rescaled image.\n",
    "        \"\"\"\n",
    "        return F.resize(img, self.size, self.interpolation)\n",
    "\n",
    "    def __repr__(self):\n",
    "        interpolate_str = self.interpolation.value\n",
    "        return self.__class__.__name__ + '(size={0}, interpolation={1})'.format(self.size, interpolate_str)\n",
    " ```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a1b56454-8e43-4aed-9eca-0dbf4d0cd0dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(768, 512)\n",
      "tensor([[[0.3137, 0.3137, 0.3176,  ..., 0.3137, 0.3137, 0.3020],\n",
      "         [0.3176, 0.3176, 0.3176,  ..., 0.3098, 0.3137, 0.3020],\n",
      "         [0.3216, 0.3216, 0.3176,  ..., 0.3059, 0.3137, 0.3059],\n",
      "         ...,\n",
      "         [0.3412, 0.3373, 0.3373,  ..., 0.0196, 0.2196, 0.3608],\n",
      "         [0.3412, 0.3373, 0.3373,  ..., 0.3490, 0.3373, 0.3373],\n",
      "         [0.3412, 0.3373, 0.3373,  ..., 0.3529, 0.3137, 0.3216]],\n",
      "\n",
      "        [[0.5922, 0.5922, 0.5961,  ..., 0.5922, 0.5922, 0.5804],\n",
      "         [0.5961, 0.5961, 0.5961,  ..., 0.5882, 0.5922, 0.5804],\n",
      "         [0.6000, 0.6000, 0.5961,  ..., 0.5843, 0.5922, 0.5843],\n",
      "         ...,\n",
      "         [0.6275, 0.6235, 0.6235,  ..., 0.1020, 0.4157, 0.6157],\n",
      "         [0.6275, 0.6235, 0.6235,  ..., 0.5373, 0.5882, 0.6078],\n",
      "         [0.6275, 0.6235, 0.6235,  ..., 0.6392, 0.6275, 0.6275]],\n",
      "\n",
      "        [[0.9137, 0.9137, 0.9176,  ..., 0.9137, 0.9137, 0.9020],\n",
      "         [0.9176, 0.9176, 0.9176,  ..., 0.9098, 0.9137, 0.9020],\n",
      "         [0.9216, 0.9216, 0.9176,  ..., 0.9059, 0.9137, 0.9059],\n",
      "         ...,\n",
      "         [0.9294, 0.9255, 0.9255,  ..., 0.1961, 0.6353, 0.9059],\n",
      "         [0.9294, 0.9255, 0.9255,  ..., 0.7922, 0.9098, 0.9451],\n",
      "         [0.9294, 0.9255, 0.9255,  ..., 0.9412, 0.9569, 0.9373]]])\n"
     ]
    }
   ],
   "source": [
    "# Resize - 1 - åŒå‚æ•° ===================================>   ç›´æ¥å°†ä¼ å…¥çš„å›¾ç‰‡ç¼©æ”¾æˆå¡«å…¥çš„æ•°ç»„çš„æ ¼å¼ \n",
    "print(img.size) \n",
    "trans_resize = transforms.Resize((512, 512))\n",
    "# img: PIL  =>  resize =>  img_resize : PIL \n",
    "img_resize = trans_resize(img)\n",
    "# img_resize : PIL  =>  totensor  =>   img_resize  tensor\n",
    "img_resize = trans_totensor(img_resize)\n",
    "print(img_resize)\n",
    "\n",
    "writer.add_image(\"Image Resize Test\", img_resize, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d57d268e-3476-4341-9183-d9dec2f111b0",
   "metadata": {},
   "source": [
    "### 6. .Composeæ–¹æ³•\n",
    "---\n",
    "```python\n",
    "class Compose:\n",
    "    \"\"\"Composes several transforms together. This transform does not support torchscript.\n",
    "    Please, see the note below.\n",
    "\n",
    "    Args:\n",
    "        transforms (list of ``Transform`` objects): list of transforms to compose.\n",
    "\n",
    "    Example:\n",
    "        >>> transforms.Compose([\n",
    "        >>>     transforms.CenterCrop(10),\n",
    "        >>>     transforms.ToTensor(),\n",
    "        >>> ])\n",
    "\n",
    "    .. note::\n",
    "        In order to script the transformations, please use ``torch.nn.Sequential`` as below.\n",
    "\n",
    "        >>> transforms = torch.nn.Sequential(\n",
    "        >>>     transforms.CenterCrop(10),\n",
    "        >>>     transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225)),\n",
    "        >>> )\n",
    "        >>> scripted_transforms = torch.jit.script(transforms)\n",
    "\n",
    "        Make sure to use only scriptable transformations, i.e. that work with ``torch.Tensor``, does not require\n",
    "        `lambda` functions or ``PIL.Image``.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, transforms):\n",
    "        self.transforms = transforms\n",
    "\n",
    "    def __call__(self, img):\n",
    "        for t in self.transforms:\n",
    "            img = t(img)\n",
    "        return img\n",
    "\n",
    "    def __repr__(self):\n",
    "        format_string = self.__class__.__name__ + '('\n",
    "        for t in self.transforms:\n",
    "            format_string += '\\n'\n",
    "            format_string += '    {0}'.format(t)\n",
    "        format_string += '\\n)'\n",
    "        return format_string\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "03e84f43-d21c-4002-9670-f494b15e7972",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Composeç›¸å½“äºå°±æ˜¯æŠŠtransformsçš„å¾ˆå¤šæ“ä½œæ‰“åŒ…åœ¨ä¸€èµ·å†å¯¹è¾“å…¥imgå¯¹è±¡è¿›è¡Œå¤„ç†\n",
    "# åé¢å‚æ•°çš„è¾“å…¥æ˜¯å‰ä¸€ä¸ªå‚æ•°çš„è¾“å‡º\n",
    "trans_resize_2 = transforms.Resize(512)                                                                          #  Resizeåªå¡«å…¥ä¸€ä¸ªå‚æ•°çš„æ—¶å€™çš„ç¼©æ”¾æ–¹å¼æ˜¯ä¸æ”¹å˜é•¿å®½æ¯”çš„ç­‰æ¯”ç¼©æ”¾ï¼Œå¹¶ä¸”å°†è¾ƒçŸ­çš„è¾¹ç¼©æ”¾è‡³512\n",
    "trans_compose = transforms.Compose([trans_resize_2,  trans_totensor])      #  img  ===>  .Resize(512)  ===>  .ToTensor  ===>  img_resize_2\n",
    "img_resize_2 = trans_compose(img)\n",
    "writer.add_image(\"Compose Test\", img_resize_2, 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "008cb196-ecf8-4781-8e50-180fd4ba7b75",
   "metadata": {},
   "source": [
    "### 7. .RandomCropæ–¹æ³•\n",
    "---\n",
    "```python\n",
    "class RandomCrop(torch.nn.Module):\n",
    "    \"\"\"Crop the given image at a random location.\n",
    "    If the image is torch Tensor, it is expected\n",
    "    to have [..., H, W] shape, where ... means an arbitrary number of leading dimensions,\n",
    "    but if non-constant padding is used, the input is expected to have at most 2 leading dimensions\n",
    "\n",
    "    Args:\n",
    "        size (sequence or int): Desired output size of the crop. If size is an\n",
    "            int instead of sequence like (h, w), a square crop (size, size) is\n",
    "            made. If provided a sequence of length 1, it will be interpreted as (size[0], size[0]).\n",
    "        padding (int or sequence, optional): Optional padding on each border\n",
    "            of the image. Default is None. If a single int is provided this\n",
    "            is used to pad all borders. If sequence of length 2 is provided this is the padding\n",
    "            on left/right and top/bottom respectively. If a sequence of length 4 is provided\n",
    "            this is the padding for the left, top, right and bottom borders respectively.\n",
    "            In torchscript mode padding as single int is not supported, use a sequence of length 1: ``[padding, ]``.\n",
    "        pad_if_needed (boolean): It will pad the image if smaller than the\n",
    "            desired size to avoid raising an exception. Since cropping is done\n",
    "            after padding, the padding seems to be done at a random offset.\n",
    "        fill (number or str or tuple): Pixel fill value for constant fill. Default is 0. If a tuple of\n",
    "            length 3, it is used to fill R, G, B channels respectively.\n",
    "            This value is only used when the padding_mode is constant.\n",
    "            Only number is supported for torch Tensor.\n",
    "            Only int or str or tuple value is supported for PIL Image.\n",
    "        padding_mode (str): Type of padding. Should be: constant, edge, reflect or symmetric. Default is constant.\n",
    "\n",
    "             - constant: pads with a constant value, this value is specified with fill\n",
    "\n",
    "             - edge: pads with the last value on the edge of the image\n",
    "\n",
    "             - reflect: pads with reflection of image (without repeating the last value on the edge)\n",
    "\n",
    "                padding [1, 2, 3, 4] with 2 elements on both sides in reflect mode\n",
    "                will result in [3, 2, 1, 2, 3, 4, 3, 2]\n",
    "\n",
    "             - symmetric: pads with reflection of image (repeating the last value on the edge)\n",
    "\n",
    "                padding [1, 2, 3, 4] with 2 elements on both sides in symmetric mode\n",
    "                will result in [2, 1, 1, 2, 3, 4, 4, 3]\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    @staticmethod\n",
    "    def get_params(img: Tensor, output_size: Tuple[int, int]) -> Tuple[int, int, int, int]:\n",
    "        \"\"\"Get parameters for ``crop`` for a random crop.\n",
    "\n",
    "        Args:\n",
    "            img (PIL Image or Tensor): Image to be cropped.\n",
    "            output_size (tuple): Expected output size of the crop.\n",
    "\n",
    "        Returns:\n",
    "            tuple: params (i, j, h, w) to be passed to ``crop`` for random crop.\n",
    "        \"\"\"\n",
    "        w, h = F._get_image_size(img)\n",
    "        th, tw = output_size\n",
    "\n",
    "        if h + 1 < th or w + 1 < tw:\n",
    "            raise ValueError(\n",
    "                \"Required crop size {} is larger then input image size {}\".format((th, tw), (h, w))\n",
    "            )\n",
    "\n",
    "        if w == tw and h == th:\n",
    "            return 0, 0, h, w\n",
    "\n",
    "        i = torch.randint(0, h - th + 1, size=(1, )).item()\n",
    "        j = torch.randint(0, w - tw + 1, size=(1, )).item()\n",
    "        return i, j, th, tw\n",
    "\n",
    "    def __init__(self, size, padding=None, pad_if_needed=False, fill=0, padding_mode=\"constant\"):\n",
    "        super().__init__()\n",
    "\n",
    "        self.size = tuple(_setup_size(\n",
    "            size, error_msg=\"Please provide only two dimensions (h, w) for size.\"\n",
    "        ))\n",
    "\n",
    "        self.padding = padding\n",
    "        self.pad_if_needed = pad_if_needed\n",
    "        self.fill = fill\n",
    "        self.padding_mode = padding_mode\n",
    "\n",
    "    def forward(self, img):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            img (PIL Image or Tensor): Image to be cropped.\n",
    "\n",
    "        Returns:\n",
    "            PIL Image or Tensor: Cropped image.\n",
    "        \"\"\"\n",
    "        if self.padding is not None:\n",
    "            img = F.pad(img, self.padding, self.fill, self.padding_mode)\n",
    "\n",
    "        width, height = F._get_image_size(img)\n",
    "        # pad the width if needed\n",
    "        if self.pad_if_needed and width < self.size[1]:\n",
    "            padding = [self.size[1] - width, 0]\n",
    "            img = F.pad(img, padding, self.fill, self.padding_mode)\n",
    "        # pad the height if needed\n",
    "        if self.pad_if_needed and height < self.size[0]:\n",
    "            padding = [0, self.size[0] - height]\n",
    "            img = F.pad(img, padding, self.fill, self.padding_mode)\n",
    "\n",
    "        i, j, h, w = self.get_params(img, self.size)\n",
    "\n",
    "        return F.crop(img, i, j, h, w)\n",
    "\n",
    "    def __repr__(self):\n",
    "        return self.__class__.__name__ + \"(size={0}, padding={1})\".format(self.size, self.padding)\n",
    "    \n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "b64b5cb0-e078-4b2b-9ae2-5f8ce6080185",
   "metadata": {},
   "outputs": [],
   "source": [
    "# RandomCrop\n",
    "trans_randomcrop = transforms.RandomCrop((200, 400))\n",
    "trans_compose2 = transforms.Compose([trans_randomcrop, trans_totensor])\n",
    "for _ in range(10):\n",
    "    img_crop = trans_compose2(img)\n",
    "    writer.add_image(\"RandomCrop Test\", img_crop, _)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c84c0278-e683-4857-9cdc-436d9cb2de03",
   "metadata": {},
   "source": [
    "*tips: TensorBoard,æ•…åæ€ä¹‰, ğŸ‰è¾“å…¥tensoræ ¼å¼çš„æ•°æ®æ‰èƒ½è¿›è¡Œç°å®*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9269119e-7e9b-4ba1-9805-7f53498cba40",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  },
  "toc-autonumbering": false,
  "toc-showmarkdowntxt": false
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
